{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.**\n",
        "\n",
        "--> Ensemble Learning in machine learning is a technique where multiple models (called base learners) are combined to solve the same problem and improve overall performance.\n",
        "\n",
        "**Key Idea**\n",
        "\n",
        "The main idea is:\n",
        "\n",
        "**A group of weak or diverse models can work together to produce a stronger, more accurate model.**\n",
        "\n",
        "* Instead of relying on a single model, ensemble methods reduce errors by:\n",
        "\n",
        "* Averaging predictions (reduces variance)\n",
        "\n",
        "* Correcting mistakes of previous models (reduces bias)\n",
        "\n",
        "* Combining strengths of different models"
      ],
      "metadata": {
        "id": "84V76TizFiw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "--> **Difference Between Bagging and Boosting**\n",
        "\n",
        "**Bagging:**\n",
        "\n",
        "* Models are trained independently and in parallel.\n",
        "\n",
        "* Reduces variance.\n",
        "\n",
        "* Example: Random Forest.\n",
        "\n",
        "**Boosting:**\n",
        "\n",
        "* Models are trained sequentially, each correcting previous errors.\n",
        "\n",
        "* Reduces bias.\n",
        "\n",
        "* Example: AdaBoost."
      ],
      "metadata": {
        "id": "paaJHHrYGGt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "--> Bootstrap sampling is a technique where multiple datasets are created by randomly sampling with replacement from the original dataset.\n",
        "\n",
        "**Role in Bagging.**\n",
        "\n",
        "* Each model is trained on a different bootstrap sample.\n",
        "\n",
        "* This creates diversity among models.\n",
        "\n",
        "* Their predictions are then averaged (or majority voted)."
      ],
      "metadata": {
        "id": "QXrF-oy0GiAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.  What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?**\n",
        "\n",
        "--> Out-of-Bag (OOB) samples are the data points not selected in a bootstrap sample when training models in bagging methods like Random Forest.\n",
        "\n",
        "**How OOB Score is Used**\n",
        "\n",
        "* Each model is tested on its own OOB samples.\n",
        "\n",
        "* Predictions from all models are combined.\n",
        "\n",
        "* The overall accuracy on OOB samples is called the OOB score."
      ],
      "metadata": {
        "id": "p9CYaGIRG8W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.**\n",
        "\n",
        "--> **Feature Importance: Single Tree vs. Random Forest**\n",
        "\n",
        "Single Decision Tree:\n",
        "\n",
        "* Importance is based on how much each feature reduces impurity.\n",
        "\n",
        "* Can be unstable (high variance).\n",
        "\n",
        "* Sensitive to small data changes.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "* Averages feature importance over many trees.\n",
        "\n",
        "* More stable and reliable.\n",
        "\n",
        "* Less prone to overfitting."
      ],
      "metadata": {
        "id": "Ucv5aNO-HSc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to:**\n",
        "# Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# Train a Random Forest Classifier\n",
        "# Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "\n",
        "top5 = importances.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkgwLLLvInc0",
        "outputId": "45949f88-dc1b-4d97-ac66-239bd7f640ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XAhxgs5JLOr",
        "outputId": "e7feaa2b-7281-49c4-c604-afcaf8eb79c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# Train a Random Forest Classifier\n",
        "# Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# Print the best parameters and final accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCt4iHxzJkNL",
        "outputId": "c0dc57e5-3ee1-486c-fc03-420e4578e0a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Write a Python program to:\n",
        "# Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcYXu-QEJ5h8",
        "outputId": "5ab132a0-9699-4453-e498-a8befd3324dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n",
        "\n",
        "**You decide to use ensemble techniques to increase model performance.**\n",
        "\n",
        "**Explain your step-by-step approach to:**\n",
        "* **Choose between Bagging or Boosting**\n",
        "* **Handle overfitting**\n",
        "* **Select base models**\n",
        "* **Evaluate performance using cross-validation**\n",
        "* **Justify how ensemble learning improves decision-making in this real-world\n",
        "context.**\n",
        "\n",
        "\n",
        "--> **Choose Between Bagging or Boosting**\n",
        "\n",
        "* Bagging (e.g., Random Forest) is ideal if the dataset is large, noisy, and prone to high variance, as it reduces variance by averaging multiple models.\n",
        "\n",
        "* Boosting (e.g., XGBoost, LightGBM, AdaBoost) is preferred if the dataset has complex patterns and class imbalance, since boosting sequentially corrects errors and reduces bias.\n",
        "\n",
        "* Step: Start by exploring data distribution and model variance. For highly imbalanced loan defaults, Boosting often performs better.\n",
        "\n",
        "**Handle Overfitting**\n",
        "\n",
        "* Regularization: Use hyperparameters like max_depth, min_samples_leaf (for trees) to avoid overly complex models.\n",
        "\n",
        "* Early stopping: Especially in boosting (XGBoost/LightGBM) to stop when performance on validation stops improving.\n",
        "\n",
        "* Feature selection: Remove redundant or irrelevant features.\n",
        "\n",
        "* Ensemble methods: Bagging itself reduces overfitting by averaging predictions; boosting can overfit, so careful tuning is essential.\n",
        "\n",
        "**Select Base Models**\n",
        "\n",
        "-> Common choices for structured financial data:\n",
        "\n",
        "* Decision Trees – easy to interpret, good with categorical and numerical features.\n",
        "\n",
        "* Logistic Regression – useful as a simple base model, especially in boosting.\n",
        "\n",
        "* Gradient Boosting Trees – strong default choice for tabular data.\n",
        "\n",
        "* Step: Use trees for both bagging and boosting; optionally try hybrid ensembles (stacking) combining trees + logistic regression for interpretability.\n",
        "\n",
        "**Evaluate Performance Using Cross-Validation**\n",
        "\n",
        "* K-Fold CV: Split data into k folds (e.g., 5 or 10) to get reliable performance estimates.\n",
        "\n",
        "* Metrics for loan default: Use ROC-AUC for imbalanced data, along with accuracy, precision, recall, and F1-score.\n",
        "\n",
        "* Hyperparameter tuning: Combine CV with GridSearchCV or RandomizedSearchCV to select best parameters.\n",
        "\n",
        "**Justify How Ensemble Learning Improves Decision-Making**\n",
        "\n",
        "* Reduced risk of wrong decisions: Ensemble predictions are more robust and accurate than a single model, reducing false defaults or missed defaults.\n",
        "\n",
        "* Captures complex patterns: Boosting identifies subtle relationships in customer behavior that a single model may miss.\n",
        "\n",
        "* Stability: Bagging reduces variance, so decisions are consistent across different data samples.\n",
        "\n",
        "* Business impact: Higher prediction accuracy means better credit risk assessment, lowering financial losses and improving regulatory compliance."
      ],
      "metadata": {
        "id": "5JNTvCkWKYrb"
      }
    }
  ]
}